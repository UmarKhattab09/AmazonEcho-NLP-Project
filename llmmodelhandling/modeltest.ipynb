{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7061e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "model_name = \"E:\\\\models\\\\pleasRAG\"\n",
    "token = os.getenv(\"HFTOKEN\")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9c883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khatt\\Documents\\AmazonEcho-NLP-Project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM \n",
    "from transformers import AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4fb237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2365d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "model_name = \"E:/models/gte-small\"\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    multi_process=False,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e6720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up one level (or more depending on where your .ipynb file is)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))  # adjust if needed\n",
    "sys.path.append(project_root)\n",
    "from WebsiteScrap import WebsiteScrapper\n",
    "web = WebsiteScrapper(\"https://www.amazon.com/Amazon-Basics-Everyday-Plates-Disposable/dp/B0C2CY22B8/?_encoding=UTF8&pd_rd_w=APjaP&content-id=amzn1.sym.f2128ffe-3407-4a64-95b5-696504f68ca1&pf_rd_p=f2128ffe-3407-4a64-95b5-696504f68ca1&pf_rd_r=YXMEXN435CNCQZWD512A&pd_rd_wg=7Izqy&pd_rd_r=3fbb0d02-8f22-4b11-ac45-f11f9ef282d6&ref_=pd_hp_d_btf_crs_zg_bs_284507&th=1\")\n",
    "dfname = web.websitename()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fbf6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "envpath = \"../.env\"\n",
    "load_dotenv(dotenv_path=envpath)\n",
    "PINECONE = os.getenv(\"PINECONE\")\n",
    "pc = Pinecone(api_key=PINECONE)\n",
    "index = pc.Index(\"vectorized\")\n",
    "user_input = input(\"User: \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccdc0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What do people thinki about the product\"\n",
    "vectorized_input = embedding_model.embed_query(user_input)\n",
    "\n",
    "context = index.query(\n",
    "    namespace=\"echo\",\n",
    "    vector=vectorized_input,\n",
    "    top_k=2,\n",
    "    include_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56631e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context['matches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fd73553",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [context['matches'][i]['metadata']['text'] for i in range(len(context['matches']))]\n",
    "combinedtext = str(\".\".join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae78d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an assistant that is answer people questions regarding there userinput.\n",
    "You are also provided context from a source that relates to the userinput\n",
    "Please think and answer based on the userinput and the context that you are given.\n",
    "Also provide the answer in a json format\n",
    "{\"userinput\":{},\n",
    " \"context\" : \n",
    "\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb6de1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "answer = pipe(prompt.format(userinput = user_input,context=combinedtext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d89412a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that is answer people questions regarding there userinput.\n",
      "What do people thinki about the product:\n",
      "You are also provided context from a source that relates to the userinput\n",
      "Awesome product! However, it does take time for Alexa to recognize other language when asked to play Spanish music and after a while sheâ€™ll turn off by it self..I really enjoy itI\n",
      "Please think and answer based on the userinput and the context that you are given.\n",
      "You will be asked to provide a name for the product.\n",
      "I need to know what do you think about something.\n",
      "I have already answered this question.\n",
      "I need to know what do you think about something.\n",
      "I have already answered this question.\n",
      "I want to know what do you think about something.\n",
      "I have already answered this question.\n",
      "I want to know what do you think about something.\n",
      "I have already answered this question.\n",
      "I want to know what do you think about something.\n",
      "I have already answered this question.\n",
      "I want to know what do you think about something.\n",
      "I have already answered this question.\n",
      "I want to know what do you think about something.\n",
      "I have already answered this question.\n",
      "I want to know what do you think about something.\n",
      "I have already answered this question.\n",
      "I want to know what do you think about something.\n",
      "I have already answered this question.\n",
      "I want to know what do you think about something.\n",
      "I have already answered this question.\n",
      "I want to know what do you think about something.\n",
      "I have already answered this question.\n",
      "2 The first of these, the \"new\" user interface, is a set of tools that,\n"
     ]
    }
   ],
   "source": [
    "print(answer[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e9782e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from WebsiteScrap import WebsiteScrapper\n",
    "from embeddedstuff import Embedded\n",
    "from transformers import AutoModelForCausalLM \n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "class llmmodel:\n",
    "    def __init__(self,user_input,url):\n",
    "        self.model_name=\"E:\\\\models\\\\pleasRAG\"\n",
    "        self.user_input = user_input\n",
    "        self.url = url\n",
    "\n",
    "    def llmmodel(self):\n",
    "        model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            return_full_text=True\n",
    "        )\n",
    "\n",
    "        return pipe\n",
    "        \n",
    "\n",
    "    def context(self):\n",
    "        web = WebsiteScrapper\n",
    "        emb = Embedded\n",
    "        embedding_model=emb.load_embmodel()\n",
    "        dfname = web.websitename\n",
    "        PINECONE = os.getenv(\"PINECONE\")\n",
    "        pc = Pinecone(api_key=PINECONE)\n",
    "        index = pc.Index(\"amazon\")\n",
    "        \n",
    "        vectorized_input = embedding_model.embed_query(self.user_input)\n",
    "        context = index.query(\n",
    "            namespace=dfname,\n",
    "            vector=vectorized_input,\n",
    "            top_k=2,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        combinedcontext = [context['matches'][i]['metadata']['text'] for i in range(len(context['matches']))]\n",
    "        combinedcontext=str(\".\".join(combinedcontext))\n",
    "        return combinedcontext\n",
    "\n",
    "    def feedback(self):\n",
    "        contextfromdb = self.context()\n",
    "        prompt = \"\"\"What do yu think about the product based on the reviews \n",
    "        {reviews}\"\"\"\n",
    "        answer = pipe(prompt.format(review = contextfromdb))\n",
    "\n",
    "        return answer[0]['generatedtext']\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
